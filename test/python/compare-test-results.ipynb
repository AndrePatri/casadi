{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, dirname, basename, splitext, abspath\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import timedelta\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "this_folder = dirname(__file__) if '__file__' in globals() else abspath('')\n",
    "root_folder = dirname(dirname(this_folder))\n",
    "def get_test_result_folder(testname = 'baseline'):\n",
    "    return join(root_folder, 'build', 'testresults', testname, 'CUTEst')\n",
    "\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return f'color: {color}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(folder):\n",
    "    output_files = glob(join(folder, '*.yaml'))\n",
    "    raw_data = {}\n",
    "    for filename in output_files:\n",
    "        with open(filename, 'r') as f:\n",
    "            all_content = yaml.safe_load_all(f)\n",
    "            content = next(all_content)\n",
    "            name = splitext(basename(filename))[0]\n",
    "            raw_data[name] = content\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-provider",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_data(raw_data):\n",
    "    data = []\n",
    "    for name, content in raw_data.items():\n",
    "        element = {\n",
    "            'name': name,\n",
    "            'status': content['status'],\n",
    "#             'time': timedelta(seconds=content['elapsed time']),\n",
    "            'time': float(content['elapsed time']),\n",
    "            'inner iterations': content['inner iterations'],\n",
    "            'outer iterations': content['outer iterations'],\n",
    "            'inner convergence failures': content['inner convergence failures'],\n",
    "            'f': float(content['f']),\n",
    "            'ε': float(content['ε']),\n",
    "            'δ': float(content['δ']),\n",
    "            'f evaluations': content['counters']['f'],\n",
    "            'grad_f evaluations': content['counters']['grad_f'],\n",
    "            'g evaluations': content['counters']['g'],\n",
    "            'grad_g evaluations': content['counters']['grad_g'],\n",
    "            'linesearch failures': content['linesearch failures'],\n",
    "            'L-BFGS failures': content['L-BFGS failures'],\n",
    "            'L-BFGS rejected': content['L-BFGS rejected'],\n",
    "        }\n",
    "        data.append(element)\n",
    "    df = pd.DataFrame(data)\n",
    "    # df.sort_values(['status', 'inner iterations'], inplace=True, ignore_index=True)\n",
    "    # df.sort_values(['name'], inplace=True, ignore_index=True)\n",
    "    df.set_index('name', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    df['rel linesearch failures'] = df['linesearch failures'] / df['inner iterations']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = get_test_result_folder('baseline')\n",
    "lslp_folder = get_test_result_folder('linesearch-lipschitz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-there",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_raw = load_raw_data(base_folder)\n",
    "lslp_raw = load_raw_data(lslp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = convert_data(base_raw)\n",
    "lslp_df = convert_data(lslp_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-senator",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lslp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(df):\n",
    "    conv = df['status'].value_counts()['Converged']\n",
    "    tot = df['status'].count()\n",
    "    tot_time = df['time'].sum()\n",
    "    conv_time = df.where(df['status'] == 'Converged')['time'].sum()\n",
    "    print(f'Converged:      {conv}/{tot} = {100*conv/tot}%')\n",
    "    print(f'Total time:     {tot_time}')\n",
    "    print(f'Converged time: {conv_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline\\n---\\n')\n",
    "df_stats(base_df)\n",
    "print('\\n')\n",
    "print('Linesearch with Lipschitz check\\n---\\n')\n",
    "df_stats(lslp_df)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(a, b, columns):\n",
    "#     res = pd.DataFrame()\n",
    "#     for i, df in enumerate(dfs):\n",
    "#         res[f'{column} {i}'] = df[column]\n",
    "#     return res\n",
    "    res = a[columns].join(b[columns], lsuffix=' 0', rsuffix=' 1')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-multimedia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "cmp = compare_results(base_df, lslp_df, ['status', 'f', 'ε', 'δ']) #  'inner iterations', 'outer iterations',\n",
    "cmp['improvement'] = cmp['f 0'] - cmp['f 1']\n",
    "cmp['rel improvement'] = cmp['improvement'] / cmp['f 0']\n",
    "cmp.style.applymap(color_negative_red, subset=['improvement', 'rel improvement']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-differential",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "worse_f = cmp[(cmp['improvement'] < 0) & (cmp['status 1'] == 'Converged')]\n",
    "print(f'{len(worse_f)} tests got worse results')\n",
    "tol = 1e-5\n",
    "really_worse_f = worse_f[abs(worse_f['rel improvement']) > tol]\n",
    "print(f'{len(really_worse_f)} tests got significantly worse')\n",
    "really_worse_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-uniform",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statusses = base_df[['status']].join(lslp_df[['status']], lsuffix=' 0', rsuffix=' 1')\n",
    "not_conv_to_conv = (statusses['status 0'] != 'Converged') & (statusses['status 1'] == 'Converged')\n",
    "conv_to_not_conv = (statusses['status 0'] == 'Converged') & (statusses['status 1'] != 'Converged')\n",
    "\n",
    "print(f'{len(base_df[not_conv_to_conv])} tests that didn\\'t converge before do converge after the change')\n",
    "print(f'{len(base_df[conv_to_not_conv])} tests that converged before no longer converge after the change')\n",
    "\n",
    "display(base_df[conv_to_not_conv])\n",
    "display(lslp_df[conv_to_not_conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-abraham",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(base_df[not_conv_to_conv])\n",
    "# display(lslp_df[not_conv_to_conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-product",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "both_converged = (base_df['status'] == 'Converged') & (lslp_df['status'] == 'Converged')\n",
    "cmp = compare_results(base_df[both_converged], lslp_df[both_converged], ['status', 'time', 'f evaluations', 'grad_f evaluations'])\n",
    "cmp['time improvement'] = cmp['time 0'] - cmp['time 1']\n",
    "cmp['rel time improvement'] = cmp['time improvement'] / cmp['time 0']\n",
    "cmp['f eval improvement'] = cmp['f evaluations 0'] - cmp['f evaluations 1']\n",
    "cmp['rel f eval improvement'] = cmp['f eval improvement'] / cmp['f evaluations 0']\n",
    "cmp['grad_f eval improvement'] = cmp['grad_f evaluations 0'] - cmp['grad_f evaluations 1']\n",
    "cmp['rel grad_f eval improvement'] = cmp['grad_f eval improvement'] / cmp['grad_f evaluations 0']\n",
    "print(f\"Net time improvement:      {cmp['time improvement'].sum()}\")\n",
    "print(f\"Relative time improvement: {cmp['rel time improvement'].sum()}\")\n",
    "print(f\"Net f eval improvement:      {cmp['f eval improvement'].sum()}\")\n",
    "print(f\"Relative f eval improvement: {cmp['rel f eval improvement'].sum()}\")\n",
    "print(f\"Net grad_f eval improvement:      {cmp['grad_f eval improvement'].sum()}\")\n",
    "print(f\"Relative grad_f eval improvement: {cmp['rel grad_f eval improvement'].sum()}\")\n",
    "print('positive is good, negative is bad')\n",
    "\n",
    "cmp.style.applymap(color_negative_red, subset=['time improvement', 'rel time improvement', 'f eval improvement', 'rel f eval improvement', 'grad_f eval improvement', 'rel grad_f eval improvement']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-abuse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmp = compare_results(base_df[both_converged], lslp_df[both_converged], ['status', 'linesearch failures', 'L-BFGS failures', 'L-BFGS rejected'])\n",
    "cmp['ls imprv'] = cmp['linesearch failures 0'] - cmp['linesearch failures 1']\n",
    "cmp['rel ls imprv'] = cmp['ls imprv'] / cmp['linesearch failures 0']\n",
    "cmp['lbfgs imprv'] = cmp['L-BFGS failures 0'] - cmp['L-BFGS failures 1']\n",
    "cmp['rel lbfgs imprv'] = cmp['lbfgs imprv'] / cmp['L-BFGS failures 0']\n",
    "cmp['lbfgs rej imprv'] = cmp['L-BFGS rejected 0'] - cmp['L-BFGS rejected 1']\n",
    "cmp['rel lbfgs rej imprv'] = cmp['lbfgs rej imprv'] / cmp['L-BFGS rejected 0']\n",
    "print(f\"Net linesearch failures improvement:    {cmp['ls imprv'].sum()}\")\n",
    "print(f\"Relative linesearch improvement:        {cmp['rel ls imprv'].sum()}\")\n",
    "print(f\"Net L-BFGS failures improvement:        {cmp['lbfgs imprv'].sum()}\")\n",
    "print(f\"Relative L-BFGS failures improvement:   {cmp['rel lbfgs imprv'].sum()}\")\n",
    "print(f\"Net L-BFGS rejections improvement:      {cmp['lbfgs rej imprv'].sum()}\")\n",
    "print(f\"Relative L-BFGS rejections improvement: {cmp['rel lbfgs rej imprv'].sum()}\")\n",
    "\n",
    "cmp.style.applymap(color_negative_red, subset=['ls imprv', 'rel ls imprv', 'lbfgs imprv', 'rel lbfgs imprv', 'lbfgs rej imprv', 'rel lbfgs rej imprv']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-annotation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
